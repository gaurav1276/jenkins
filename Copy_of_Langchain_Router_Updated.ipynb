{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurav1276/jenkins/blob/master/Copy_of_Langchain_Router_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain ü¶úüîó:  Routing**"
      ],
      "metadata": {
        "id": "MjhfGDRHt_in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **[Reference](https://python.langchain.com/docs/how_to/routing/)**"
      ],
      "metadata": {
        "id": "3VTu12UyxkhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "ow0D5JMrd_Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0b074a-4f14-4393-b86a-5bfa609f5212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai<2.0.0,>=1.54.0 (from langchain-openai)\n",
            "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-core, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "Successfully installed langchain-core-0.3.15 langchain-openai-0.2.6 openai-1.54.3 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os"
      ],
      "metadata": {
        "id": "ONj0t8WTfsBN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/openAikey.txt')\n",
        "api_key = f.read()\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "openai.api_key= os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "eK9KcnIbfuCF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "-AYTyQDpQxXE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temprature Zero because we are doing factual things.\n",
        "Factual means i will provode query i need factial answers"
      ],
      "metadata": {
        "id": "z7XxhuO6n6w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = \"gpt-4o-mini\" # This is a chat model\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)"
      ],
      "metadata": {
        "id": "HQ4GJ5CPQt3Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is my prompt. Given question below"
      ],
      "metadata": {
        "id": "UyeT1r2_onsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_classify = PromptTemplate.from_template(\"\"\"Given the user question below, classify it as either being about\n",
        " `Physics`, `Math`, `Computer Science`.Do not respond with more than one word.\n",
        " <question> {question} </question> Classification:\"\"\")\n"
      ],
      "metadata": {
        "id": "w4HVAufXR_TV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_classify"
      ],
      "metadata": {
        "id": "HntYnRVefdA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161498f2-ff54-46e7-a343-94ff1580d52f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Given the user question below, classify it as either being about\\n `Physics`, `Math`, `Computer Science`.Do not respond with more than one word.\\n <question> {question} </question> Classification:')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are doing 3 things below chain\n",
        "* Prompt Classify - Prompted will be curated\n",
        "* llm - curated prompt will be passed into language model (llm)\n",
        "* String Output Parser - due to StrOutputParser give output wether question is from matths, physics, computer science otherwise output has lot more things"
      ],
      "metadata": {
        "id": "CKQ1LAbupA9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_classify  | llm | StrOutputParser()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "p_gSga7TS0ob"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7UhZMScnqiCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using shell expression language\n",
        "prompt_classify = PromptTemplate.from_template(\"\"\"Given the user question below, classify it as either being about\n",
        " `Physics`, `Math`, `Computer Science`.Do not respond with more than one word.\n",
        " <question> {question} </question> Classification:\"\"\")\n",
        "\n",
        " question that we pass below will be replaced to above {question} that we created above and give output Computer science"
      ],
      "metadata": {
        "id": "ZUjqU9alqoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\":\"What's a programming language?\"})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wJvqe-T7tkYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bewlo we are creating Chat template and chains\n",
        "* 1 Template for Physics\n",
        "* 1 Chain for physics"
      ],
      "metadata": {
        "id": "xFNdi9BjrK2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{question}\n",
        "Start your answer with `Subject:Physics :`\n",
        "\"\"\"\n",
        "physics_prompt=ChatPromptTemplate.from_template(physics_template) # ChatPromptTemplate , PromptTemplate\n",
        "physics_chain = physics_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "PSHQ2YXGo9S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bewlo we are creating Chat template and chains\n",
        "* 1 Template for Math\n",
        "* 1 Chain for Math"
      ],
      "metadata": {
        "id": "E-_DhAkkrt58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{question}\n",
        "Start your answer with `Subject:Math :`\n",
        "\"\"\"\n",
        "math_prompt=ChatPromptTemplate.from_template(math_template)\n",
        "math_chain = math_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "G5NIcMcFprGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bewlo we are creating Chat template and chains\n",
        "* 1 Template for Compute Science\n",
        "* 1 Chain for Computer Science"
      ],
      "metadata": {
        "id": "alBw1qe5ry6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{question}\n",
        "Start your answer with `Subject:Computer Science :`\n",
        "\"\"\"\n",
        "\n",
        "computer_science_prompt=ChatPromptTemplate.from_template(computerscience_template)\n",
        "computer_science_chain = computer_science_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "86AoFDSTup-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Prompt"
      ],
      "metadata": {
        "id": "Gy9eL0Uyr9IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "general_prompt=ChatPromptTemplate.from_template(\n",
        "    \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Start your answer with `Subject:General :`\"\"\"\n",
        "\n",
        ")\n",
        "general_chain = general_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "7hTH8iE1p1Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, routers are used to route a query or task to the appropriate model or chain of models based on the input‚Äôs content. In your example, the route function decides which processing \"chain\" to use based on the topic of the input.\n",
        "\n",
        "The route function here is designed to route information (passed in as the info dictionary) to different chains depending on the topic key. Here's a breakdown of what it does:\n",
        "\n",
        "* Function Definition:\n",
        "`def route(info):`\n",
        "  * The function takes in info, a dictionary, which we expect to contain at least a topic key that describes the subject matter.\n",
        "\n",
        "**Checking the Topic:**\n",
        "\n",
        "* The function checks the topic of info to determine which specific chain to use.\n",
        "* The topic is checked in lowercase form (using .lower()), which helps make the routing case-insensitive.\n",
        "\n",
        "Conditional Routing\n",
        "\n",
        "\n",
        "```\n",
        "if \"physics\" in info[\"topic\"].lower():\n",
        "    return physics_chain\n",
        "elif \"math\" in info[\"topic\"].lower():\n",
        "    return math_chain\n",
        "elif \"computer science\" in info[\"topic\"].lower():\n",
        "    return computer_science_chain\n",
        "else:\n",
        "    return general_chain\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MMg4ulnOsVaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route(info):\n",
        "    if \"physics\" in info[\"topic\"].lower():\n",
        "        return physics_chain\n",
        "    elif \"math\" in info[\"topic\"].lower():\n",
        "        return math_chain\n",
        "    elif \"computer science\" in info[\"topic\"].lower():\n",
        "        return computer_science_chain\n",
        "    else:\n",
        "        return general_chain"
      ],
      "metadata": {
        "id": "fh1nAHozq0IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RunnableLambda:**\n",
        "\n",
        "* RunnableLambda is a utility in LangChain that enables custom, callable functions to be executed as part of a processing pipeline. It allows you to define arbitrary logic for routing, transforming, or processing data on-the-fly within a LangChain chain.\n",
        "* Here, RunnableLambda(route) wraps the route function we defined earlier. By using RunnableLambda, the route function can act as part of a LangChain chain, dynamically selecting which chain to use based on the conditions defined within the function.\n",
        "\n",
        "**The full_chain Pipeline:**\n",
        "\n",
        "* full_chain is a dictionary-based pipeline, which combines two components to direct data based on specific conditions.\n",
        "* \"topic\": chain: This element represents the main chain you‚Äôll be using to process data. The exact behavior of chain isn‚Äôt specified here, but it likely handles the main task based on the input topic.\n",
        "* \"question\": lambda x: x[\"question\"]: This is a simple lambda function that extracts the question key from the input dictionary. It allows the chain to access the exact question text when needed.\n",
        "\n",
        "**Piping into RunnableLambda(route):**\n",
        "\n",
        "* The use of the | operator here pipes the output from the dictionary {\"topic\": chain, \"question\": lambda x: x[\"question\"]} into RunnableLambda(route).\n",
        "* Essentially, this means that route will receive the output of the dictionary pipeline, and based on the topic, it will route the input to the appropriate chain (e.g., physics_chain, math_chain, computer_science_chain, or general_chain).\n",
        "\n",
        "**How full_chain Works:**\n",
        "\n",
        "* When you provide full_chain with an input, the pipeline first processes each key-value pair in the dictionary:\n",
        "* For the \"topic\" key, it will use the general chain.\n",
        "* For the \"question\" key, it will apply lambda x: x[\"question\"] to get the question text.\n",
        "* After processing these initial steps, the output is piped into RunnableLambda(route), which will route the input to the correct specialized chain based on the topic."
      ],
      "metadata": {
        "id": "_Hy_V_Ittv4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(route)\n",
        "full_chain"
      ],
      "metadata": {
        "id": "RaZF5wa0rXos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfdf95fd-225a-4236-9d24-68e0f0ab46ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  topic: PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Given the user question below, classify it as either being about\\n `Physics`, `Math`, `Computer Science`.Do not respond with more than one word.\\n <question> {question} </question> Classification:')\n",
              "         | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d89faffd390>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d89faf8dd50>, root_client=<openai.OpenAI object at 0x7d89faffcbb0>, root_async_client=<openai.AsyncOpenAI object at 0x7d89faffd360>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
              "         | StrOutputParser(),\n",
              "  question: RunnableLambda(lambda x: x['question'])\n",
              "}\n",
              "| RunnableLambda(route)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Processing with {\"topic\": chain, \"question\": lambda x: x[\"question\"]}:**\n",
        "\n",
        "* The full_chain is set up with a dictionary that maps two keys, \"topic\" and \"question\".\n",
        "* Since the input only has a question field ({\"question\": \"What is black body radiation?\"}), here‚Äôs what each part does:\n",
        "\n",
        "**\"question\": lambda x: x[\"question\"]:**\n",
        "* The lambda function lambda x: x[\"question\"] extracts the \"question\" field from the input. In this case, it will extract \"What is black body radiation?\".\n",
        "\n",
        "**\"topic\": chain:**\n",
        "* The \"topic\" key is processed by chain, although the input doesn‚Äôt specify a topic. If the chain object relies on the question text to infer the topic, it may use NLP-based classification or another logic to determine an appropriate topic.\n",
        "\n",
        "\n",
        "**Routing with RunnableLambda(route):**\n",
        "\n",
        "* After processing, the dictionary output is piped into RunnableLambda(route).\n",
        "The route function, wrapped in RunnableLambda, checks for specific keywords in the topic. Since the topic wasn‚Äôt explicitly specified, the route function will likely infer it based on the question text (\"What is black body radiation?\").\n",
        "If the route function is set up to recognize physics-related terms (e.g., \"radiation\"), it will select physics_chain.\n",
        "\n",
        "**Execution of the Selected Chain:**\n",
        "\n",
        "* The selected chain (e.g., physics_chain, if matched) will process the question \"What is black body radiation?\" to generate a response.\n",
        "If no keywords match, the route function defaults to general_chain, which processes the question in a more generic way."
      ],
      "metadata": {
        "id": "AIR1aKzsuyiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "With full_chain.invoke({\"question\": \"What is black body radiation?\"}), LangChain:\n",
        "\n",
        "* Extracts the question text.\n",
        "* Routes it based on the topic inferred from the content.\n",
        "* Passes it to the corresponding chain for processing."
      ],
      "metadata": {
        "id": "g_7Zj8FSverE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What is black body radiation?\"})"
      ],
      "metadata": {
        "id": "gFqN8GDmrb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "378e5167-593a-4d88-be54-925783557a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Subject: Physics: \\n\\nBlack body radiation refers to the electromagnetic radiation emitted by an idealized object known as a \"black body,\" which absorbs all incident radiation, regardless of frequency or angle. When a black body is in thermal equilibrium, it emits radiation in a characteristic spectrum that depends solely on its temperature, described by Planck\\'s law. \\n\\nAs the temperature of the black body increases, the peak wavelength of the emitted radiation shifts to shorter wavelengths, a phenomenon described by Wien\\'s displacement law. This concept is fundamental in understanding thermal radiation and has significant implications in various fields, including astrophysics and quantum mechanics.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What's a programming language?\"})"
      ],
      "metadata": {
        "id": "E4mGH__8r5M3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "9804cf7f-8b59-450b-b120-89bf6c83078a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Subject:Computer Science : \\n\\nA programming language is a formal set of instructions that can be used to produce various kinds of output, including software applications, algorithms, and data processing. It serves as a medium for humans to communicate with computers, allowing us to write code that the machine can interpret and execute.\\n\\nProgramming languages consist of syntax (rules for structure) and semantics (meaning of the statements). They can be categorized into high-level languages, which are more abstract and closer to human languages (like Python, Java, and C#), and low-level languages, which are closer to machine code (like Assembly and C).\\n\\nKey characteristics of programming languages include:\\n\\n1. **Syntax**: The set of rules that defines the combinations of symbols that are considered to be correctly structured programs.\\n2. **Semantics**: The meaning of the syntactic elements and how they are interpreted by the machine.\\n3. **Paradigms**: Different programming styles, such as procedural, object-oriented, functional, and declarative programming, which influence how problems are approached and solved.\\n4. **Abstraction**: The ability to hide complex implementation details and expose only the necessary parts to the programmer, making it easier to develop and maintain software.\\n\\nIn summary, programming languages are essential tools for software development, enabling programmers to create efficient, reliable, and maintainable code to solve complex problems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What's 1+1\"})"
      ],
      "metadata": {
        "id": "jsSERmUHsn5F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "42a80cdc-bead-4456-99fe-78397392e82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Subject:Math :  \\nTo solve the problem 1 + 1, we can break it down into its component parts:\\n\\n1. Identify the numbers involved: We have the number 1 and another number 1.\\n2. Understand the operation: The operation we are performing is addition, which combines the values of the numbers.\\n\\nNow, we can add the two numbers together:\\n1 + 1 = 2\\n\\nTherefore, the answer to the question is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What's a path integral?\"}) #"
      ],
      "metadata": {
        "id": "RPn5Cznus4L0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ec9c5f8d-4ae0-43e1-c88a-30c12a5ef263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Subject: Physics : \\n\\nA path integral is a formulation of quantum mechanics that sums over all possible paths a particle can take between two points. Instead of considering a single trajectory, the path integral approach, developed by Richard Feynman, involves calculating the contribution of every conceivable path, each weighted by a factor related to the action (a quantity that encapsulates the dynamics of the system). \\n\\nMathematically, the path integral is expressed as an integral over all paths, and it provides a powerful way to compute quantum amplitudes. This method is particularly useful in quantum field theory and has applications in various areas of physics, including statistical mechanics and string theory. The concept emphasizes the probabilistic nature of quantum mechanics and highlights the idea that particles do not follow a single, well-defined path but rather explore all possible routes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"tell me 2 facts about Taj Mahal India?\"})"
      ],
      "metadata": {
        "id": "U6qTbZRCvo54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8fbdd9b3-d88a-40f5-d414-d8445dacba7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Subject:General :  \\n1. The Taj Mahal, located in Agra, India, was commissioned by Mughal Emperor Shah Jahan in memory of his beloved wife Mumtaz Mahal, who died during childbirth. Construction began in 1632 and was completed in 1653, making it a symbol of love and devotion.  \\n\\n2. The Taj Mahal is renowned for its stunning white marble architecture, which reflects different hues throughout the day, creating a mesmerizing visual effect. It is also a UNESCO World Heritage Site and attracts millions of visitors each year, making it one of the most famous landmarks in the world.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}